{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from fxpmath import Fxp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from qkeras import QDense, QActivation\n",
    "from qkeras.quantizers import binary, ternary, quantized_relu, quantized_bits\n",
    "\n",
    "seed = 0\n",
    "width = 64\n",
    "num_layers = 2\n",
    "reuse_factor = 1\n",
    "epochs = 5\n",
    "\n",
    "weight_scaling_factor = 1\n",
    "act_scaling_factor = 1\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "saved_model_dir = f'exp_8bit_qnn'\n",
    "# delete the saved_model_dir if it exists\n",
    "if os.path.exists(saved_model_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(saved_model_dir)\n",
    "# create the saved_model_dir if it doesn't exist\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5107422   0.47070312  0.38476562  0.06933594 -0.04101562 -0.14160156\n",
      "  -0.86816406 -0.5527344  -0.86816406 -0.7832031  -0.80859375 -0.84765625\n",
      "  -0.96484375 -0.83496094  0.18652344  0.46484375]\n",
      " [ 0.7480469  -0.49804688  0.12304688  0.03613281  0.56152344  0.06640625\n",
      "   0.7236328  -0.09960938  0.7236328  -0.51464844 -0.65722656 -0.5019531\n",
      "  -0.19628906 -0.4345703   0.2626953  -0.8720703 ]\n",
      " [ 0.8544922  -0.90625    -1.2177734  -0.8486328  -1.1044922  -0.6230469\n",
      "  -0.02929688  0.19628906 -0.02929688  0.9121094   1.1347656   1.7070312\n",
      "   0.9628906   1.3779297  -1.2890625  -0.73339844]\n",
      " [ 1.2910156  -1.0869141  -1.203125   -0.84765625 -1.1044922  -0.6230469\n",
      "  -0.47851562 -0.29492188 -0.47851562  0.31933594  0.90625     1.6035156\n",
      "   0.42285156  0.796875   -1.2773438  -1.1484375 ]\n",
      " [-0.31933594  0.578125   -0.08984375 -0.04394531  0.71972656  0.7324219\n",
      "   1.7363281   1.5029297   1.7363281  -0.703125   -1.0673828  -1.015625\n",
      "  -0.38476562 -0.47558594  0.27441406 -0.27148438]]\n",
      "(166000, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the fixed-point configuration\n",
    "# For example, ap_fixed<16,8> corresponds to 16 total bits with 8 integer bits\n",
    "total_bits = 16\n",
    "integer_bits = 6\n",
    "fractional_bits = total_bits - integer_bits\n",
    "\n",
    "X_train_val = np.load('X_train_val.npy').astype(np.float32)\n",
    "X_test = np.load('X_test.npy').astype(np.float32)\n",
    "\n",
    "# convert input data to Fxp\n",
    "X_train_val_fxp = Fxp(X_train_val, signed=True, n_word=total_bits, n_frac=fractional_bits).astype(np.float32)\n",
    "X_test_fxp = Fxp(X_test, signed=True, n_word=total_bits, n_frac=fractional_bits).astype(np.float32)\n",
    "\n",
    "# save the converted data\n",
    "np.save('X_train_val_fxp.npy', X_train_val_fxp)\n",
    "np.save('X_test_fxp.npy', X_test_fxp)\n",
    "\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)\n",
    "print(X_train_val_fxp[:5])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Model\n",
    "* Two layers of MLP, width = 64\n",
    "* 8-bit quantization on weight and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_quantizer = quantized_bits(bits=8, integer=0)\n",
    "act_quantizer = quantized_relu(bits=8, integer=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "   QDense(width, input_shape=(16,), name = f'fc0',\n",
    "               kernel_quantizer = kernel_quantizer,\n",
    "               activation = act_quantizer,\n",
    "               use_bias = False)\n",
    "        )\n",
    "model.add(\n",
    "   QDense(width, name = 'fc1',\n",
    "               kernel_quantizer = kernel_quantizer,\n",
    "               activation = act_quantizer,\n",
    "               use_bias = False)\n",
    ")\n",
    "model.add(\n",
    "   QDense(5, name = 'output',\n",
    "               kernel_quantizer = kernel_quantizer,\n",
    "               activation = act_quantizer,\n",
    "               use_bias = False) \n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=adam, loss=['hinge'], metrics=['accuracy'])\n",
    "callbacks = all_callbacks(\n",
    "    stop_patience=1000,\n",
    "    lr_factor=0.5,\n",
    "    lr_patience=10,\n",
    "    lr_epsilon=0.000001,\n",
    "    lr_cooldown=2,\n",
    "    lr_minimum=0.0000001,\n",
    "    outputDir='model_3',\n",
    ")\n",
    "model.fit(\n",
    "    X_train_val_fxp,\n",
    "    y_train_val,\n",
    "    batch_size=1024,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.25,\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks.callbacks,\n",
    ")\n",
    "# Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "model.save(f'{saved_model_dir}/KERAS_check_best_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
